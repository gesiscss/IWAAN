{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is another implementation of the Biterm Topic Model as proposed <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.4032&rep=rep1&type=pdf\">in this paper</a>. It utilizes the c++ script located at BTM/. <a href=\"https://github.com/xiaohuiyan/BTM\"> Original Git repository</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.lngselection import abbreviation\n",
    "from wikiwho_wrapper import WikiWho\n",
    "from external.wikipedia import WikipediaDV, WikipediaAPI\n",
    "from metrics.conflict import ConflictManager\n",
    "import numpy as np\n",
    "import random\n",
    "from BTM.script.topicDisplay import display_topics\n",
    "import tqdm\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the page pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# ## Some Extensions ##\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %store -r the_page\n",
    "\n",
    "global lng, the_page\n",
    "\n",
    "if 'the_page' not in locals():\n",
    "    import pickle\n",
    "    print(\"Loading default data...\")\n",
    "    the_page = pickle.load(open(\"data/the_page.p\",'rb'))\n",
    "\n",
    "lng = abbreviation('English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating a wikiwho instance. Retrieving `all_content` for each token that has been changed and `revisions` for data about each revision. \n",
    "\n",
    "`ConflictManager` is used to retrieve conflicts and conflicting tokens. Dataframe `token` contains all tokens that have been changed on the page with information on revisions, original insertions and editors. `Tokens_processed` is a dataframe where each row corresponds to a revision and list of token_ids that have been changed during that revision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiwho = WikiWho(lng=lng)\n",
    "all_content = wikiwho.dv.all_content(the_page['page_id'])\n",
    "revisions = wikiwho.dv.rev_ids_of_article(the_page['page_id'])\n",
    "\n",
    "con_manager = ConflictManager(all_content.copy(), \n",
    "                                           revisions.copy(), \n",
    "                                           lng=lng, \n",
    "                                           include_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_manager.calculate()\n",
    "token = con_manager.all_actions.copy()\n",
    "tokens_processed = token[['rev_id', 'rev_time', 'editor', 'token_id', 'token']].groupby(\"rev_id\")['token_id'].apply(lambda group_series: group_series.to_numpy()).reset_index()\n",
    "tokens_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a revision contains only one token, that has been changed, this token_id is appended to either previous or next revision. `token_ids` contains a list of all unique token_ids. `X` is a a numpy array of `token_id` column. `vocab` is an array of all unique token_ids used in the article revisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tokens_processed[np.array(list(map(len,tokens_processed.token_id.values)))==1].iterrows():\n",
    "    k = random.choice([-1, 1])\n",
    "    np.append(tokens_processed.loc[i+k, 'token_id'], row['token_id'][0])\n",
    "    \n",
    "tokens_processed = tokens_processed[np.array(list(map(len,tokens_processed.token_id.values)))>1] #dropping revisions with 1 token_id\n",
    "token_ids = token[['token', 'token_id']].drop_duplicates()['token_id'].to_numpy()\n",
    "X = tokens_processed['token_id'].to_numpy()\n",
    "vocab = token[['token', 'token_id']].drop_duplicates()['token'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file <a href=\"BTM/input/input.txt\">BTM/input/input.txt</a> is going to be an input fot the script. It contains the `tokens_processed['token_id']` in a string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing input ids for the C++ model\n",
    "\n",
    "X_max = np.max([np.max(x) for x in X]) #the max token_id\n",
    "wf = open('BTM/input/input.txt', 'w')\n",
    "for x in tokens_processed['token_id']:\n",
    "    print(' '.join(map(str, [str(it) for it in x])), file=wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the <a href=\"BTM/input/vocab.txt\">BTM/input/vocab.txt</a> is going to be a vocabulary file for the script. It contains `vocab_dict` - a dictionary that maps token_ids to the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the vocab file for the C++ model\n",
    "\n",
    "vocab_dict = dict(zip(token_ids, vocab))\n",
    "with open('BTM/input/vocab.txt', 'w', newline='\\n') as f:\n",
    "    for i in vocab_dict.keys():\n",
    "        if i not in token_ids:\n",
    "            f.write(str(i) + \"\\t\" + \"\\n\")\n",
    "        else:\n",
    "            f.write(str(i) + \"\\t\" + vocab_dict[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables for the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT VARIABLES FOR THE MODEL #\n",
    "global niter, save_step, model_dir, doc_pt, voca_pt\n",
    "\n",
    "K=15   # number of topics\n",
    "alpha=0.1   #hyperparameter alpha\n",
    "beta=0.01 #hyperparameter beta\n",
    "niter=500    # number of iterations\n",
    "save_step=100    # number of steps after which to save\n",
    "\n",
    "model_dir='../output/model/' #a path for saving the output model to\n",
    "\n",
    "doc_pt='../input/input.txt' # path to the doc with token ids\n",
    "voca_pt='../input/vocab.txt' #path to the vocabulary\n",
    "\n",
    "W = X_max #vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script itself is located at <a href=\"BTM/src/btm\">BTM/src/btm</a>. To run it, firstly, the directory for the model is created at `model_dir`, then the script runs. It trains BTM over the documents and output the topics. The results will be written into the directory \"model_dir\":\n",
    "\n",
    "k20.pw_z: a K*M matrix for P(w|z), suppose K=20\n",
    "\n",
    "k20.pz: a K*1 matrix for P(z), suppose K=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the model\n",
    "%cd BTM/script\n",
    "\n",
    "!mkdir -p {model_dir}\n",
    "!make -C ../src/\n",
    "!../src/btm est {K} {W} {alpha} {beta} {niter} {save_step} {doc_pt} {model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python script <a href=\"BTM/script/topicDisplay.py\">display_topics</a> illustrates the top words for the topics and their proportions in the collection. It also shows the links to the top-3 revision diffs that have most of the tokens from that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print output\n",
    "topics, _ = display_topics(model_dir, K, voca_pt, tokens_processed, lng, the_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `bow_corpus` creates a document corpus in BoW format for calculating the overall coherence score of the trained model (it is used for the evaluation of the topic models, more to read <a href=\"https://radimrehurek.com/gensim/models/coherencemodel.html\">here</a>). The ranges are for C_V typically 0 < x < 1 and uMass -14 < x < 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global bow_corpus, dct\n",
    "\n",
    "\n",
    "def bow_corpus(token_list):\n",
    "    return [(_id, np.count_nonzero(token_list == _id)) for _id in token_list]\n",
    "\n",
    "bow_corpus = tokens_processed['token_id'].apply(bow_corpus)\n",
    "dct = Dictionary.from_corpus(bow_corpus)\n",
    "\n",
    "cm = CoherenceModel(topics=topics, corpus=bow_corpus, dictionary=dct, coherence='u_mass')\n",
    "coherence = cm.get_coherence()\n",
    "print(f'The coherence score: {coherence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cells are used for parameter tuning. The loop runs for alpha from 0.1 to 5 with step 0.3 and beta from 0.01 to 1 with step 0.3. They are commented because the results are already saved at <a href=\"BTM/script/btm_tuning_results.csv\">BTM/script/btm_tuning_results.csv</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C_v coherence measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameter tuning\n",
    "\n",
    "# # Topics range\n",
    "# min_topics = 5\n",
    "# max_topics = 50\n",
    "# step_size = 5\n",
    "# topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# # Alpha parameter\n",
    "# alpha = list(np.arange(0.1, 5, 0.3))\n",
    "\n",
    "# # Beta parameter\n",
    "# beta = list(np.arange(0.01, 1, 0.3))\n",
    "\n",
    "# model_results = { 'Topics': [],\n",
    "#                  'Alpha': [],\n",
    "#                  'Beta': [],\n",
    "#                  'Coherence_C_V': [],\n",
    "#                  'Coherence_C_U_mass': []\n",
    "#                 }\n",
    "\n",
    "# #texts in format list of lists of str (with token ids)\n",
    "# texts = tokens_processed['token_id'].apply(lambda x: x.tolist()).tolist()\n",
    "# texts = [[str(item) for item in text] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_coherence_values(k, W, a, b, tokens_processed):\n",
    "#     global niter, save_step, model_dir, doc_pt, voca_pt, bow_corpus, dct, lng, the_page, texts\n",
    "#     !mkdir -p {model_dir}\n",
    "#     !make -C ../src/\n",
    "#     !../src/btm est {k} {W} {a} {b} {niter} {save_step} {doc_pt} {model_dir}\n",
    "#     topics, _ = display_topics(model_dir, k, voca_pt, tokens_processed, lng, the_page)\n",
    "#     cm_cv = CoherenceModel(topics=topics, texts = texts, corpus=bow_corpus, dictionary=dct, coherence='c_v')\n",
    "#     cm_umass = CoherenceModel(topics=topics, corpus=bow_corpus, dictionary=dct, coherence='u_mass')\n",
    "#     c_v = cm_cv.get_coherence()\n",
    "#     c_u_mass = cm_umass.get_coherence()\n",
    "#     return c_v, c_u_mass\n",
    "\n",
    "# if 1 == 1:\n",
    "#     pbar = tqdm.tqdm(total=540)\n",
    "\n",
    "#     # iterate through number of topics\n",
    "#     for k in topics_range:\n",
    "#         # iterate through alpha values\n",
    "#         for a in alpha:\n",
    "#             # iterare through beta values\n",
    "#             for b in beta:\n",
    "#                 # get the coherence score for the given parameters\n",
    "#                 c_v, c_u_mass = compute_coherence_values(k, W, a, b, tokens_processed)\n",
    "#                 # Save the model results\n",
    "#                 model_results['Topics'].append(k)\n",
    "#                 model_results['Alpha'].append(a)\n",
    "#                 model_results['Beta'].append(b)\n",
    "#                 model_results['Coherence_C_V'].append(c_v)\n",
    "#                 model_results['Coherence_C_U_mass'].append(c_u_mass)\n",
    "\n",
    "#                 pbar.update(1)\n",
    "#     btm_cv = pd.DataFrame(model_results)#.to_csv('btm_tuning_results.csv', index=False)\n",
    "#     pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#btm_cv.to_csv('btm_tuning_results.csv', index=False) #for saving after the loop\n",
    "btm_cv = pd.read_csv('btm_tuning_results.csv')\n",
    "btm_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the parameters with highest C_U_mass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btm_cv.iloc[btm_cv['Coherence_C_U_mass'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the parameters with highest C_V:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btm_cv.iloc[btm_cv['Coherence_C_V'].idxmax()]  #high coherence because tokens mostly appear together in most of the documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand it better, the visualisation for all 3 parameters (x-axis) and coherence scores (y-axis) is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Graph of coherence U_Mass:')\n",
    "fig, axs = plt.subplots(3,1, figsize=(10, 10))\n",
    "\n",
    "btm_topics = btm_cv.groupby('Topics')['Coherence_C_U_mass'].agg('max')\n",
    "btm_alpha = btm_cv.groupby('Alpha')['Coherence_C_U_mass'].agg('max')\n",
    "btm_beta = btm_cv.groupby('Beta')['Coherence_C_U_mass'].agg('max')\n",
    "btm_topics.plot.line(x='Topics', y='Coherence_C_U_mass', ax=axs[0])\n",
    "btm_alpha.plot.line(x='Alpha', y='Coherence_C_U_mass', ax=axs[1])\n",
    "btm_beta.plot.line(x='Bets', y='Coherence_C_U_mass', ax=axs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Graph of coherence C_V:')\n",
    "fig, axs = plt.subplots(3,1, figsize=(10, 10))\n",
    "\n",
    "btm_topics = btm_cv.groupby('Topics')['Coherence_C_V'].agg('max')\n",
    "btm_alpha = btm_cv.groupby('Alpha')['Coherence_C_V'].agg('max')\n",
    "btm_beta = btm_cv.groupby('Beta')['Coherence_C_V'].agg('max')\n",
    "btm_topics.plot.line(x='Topics', y='Coherence_C_V', ax=axs[0])\n",
    "btm_alpha.plot.line(x='Alpha', y='Coherence_C_V', ax=axs[1])\n",
    "btm_beta.plot.line(x='Bets', y='Coherence_C_V', ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now training the model using the best parameters found. <b>Please update them accordingly</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model \n",
    "!mkdir -p {model_dir}\n",
    "!make -C ../src/\n",
    "!../src/btm est {10} {W} {2.8} {0.31} {niter} {save_step} {doc_pt} {model_dir} #change the numbers here for your parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the topics the best model has inferred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output of model with best parameters\n",
    "best_topics, orig_topics = display_topics(model_dir, 10, voca_pt, tokens_processed, lng, the_page)\n",
    "\n",
    "best_topics_tokens = [[vocab_dict[int(token)] for token in tokens] for tokens in best_topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordCloud visualisation of the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "i = 0\n",
    "best_topics_format = []\n",
    "for it in orig_topics.values():\n",
    "    items_split = it[0].split()\n",
    "    items_format = [(item.split(':')[0], float(item.split(':')[1])) for item in items_split]\n",
    "    best_topics_format.append((i, items_format))\n",
    "    i += 1\n",
    "\n",
    "#topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(best_topics_format[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
